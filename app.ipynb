{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install InstructorEmbedding\n",
        "!pip install PyMuPDF\n",
        "!pip install google_generativeai\n",
        "!pip install PIL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwIhcacyvMa",
        "outputId": "82ba6107-cf33-4046-90ee-2bc6cfca38b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.34)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.46)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.51)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.2)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.1 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.1)\n",
            "Requirement already satisfied: google_generativeai in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.2 in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (0.6.2)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (2.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google_generativeai) (4.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.2->google_generativeai) (1.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google_generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google_generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google_generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google_generativeai) (1.63.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google_generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google_generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google_generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google_generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google_generativeai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google_generativeai) (2.18.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google_generativeai) (1.62.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google_generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google_generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google_generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google_generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google_generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google_generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google_generativeai) (2024.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import fitz\n",
        "import sys\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "import google.generativeai as genai\n"
      ],
      "metadata": {
        "id": "saHAJV7_zsdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "genai.configure(api_key='AIzaSyDMAqL5ga6BQzk_UJwmahsFuSNz4Awm-5c')"
      ],
      "metadata": {
        "id": "e3fUZfIQ0N6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'file_1.pdf'\n",
        "lang = 'en'"
      ],
      "metadata": {
        "id": "HYGodKUU0VVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitz_doc = fitz.open(file_path)"
      ],
      "metadata": {
        "id": "WirsRW8B5ZCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
        "instructor_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",)\n",
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",model_kwargs={\"temperature\":0.55, \"max_length\":10})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUOkyNV60x69",
        "outputId": "c609f84f-5281-44ef-982b-651031ba9d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gem_pro = genai.GenerativeModel('gemini-pro')\n",
        "model_gem_pro_vis = genai.GenerativeModel('gemini-pro-vision')"
      ],
      "metadata": {
        "id": "PaebOZjy3-FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_en(raw_input):\n",
        "    prompt = \"Convert this to english - \"+raw_input\n",
        "    response = model_gem_pro.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "def translate_to_ja(raw_input):\n",
        "    prompt = \"Convert this to Japanese - \"+raw_input\n",
        "    response = model_gem_pro.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "rnC-Zx993fP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_reader = PdfReader(file_path)\n",
        "\n",
        "text_docs = []\n",
        "\n",
        "for i in range(len(pdf_reader.pages)):\n",
        "  if lang == 'en':\n",
        "    split_text = text_splitter.split_text(pdf_reader.pages[i].extract_text())\n",
        "  else:\n",
        "    split_text = text_splitter.split_text(translate_to_en(pdf_reader.pages[i].extract_text()))\n",
        "  for j in split_text:\n",
        "    doc = Document(page_content = j,metadata = {\"index\":i+1})\n",
        "    text_docs.append(doc)"
      ],
      "metadata": {
        "id": "YHk3JepM0-WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index_text = FAISS.from_documents(text_docs,instructor_embeddings)"
      ],
      "metadata": {
        "id": "jP4TyyME5C6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    faiss_index_text.as_retriever(search_kwargs={'k': 5}),\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QG-JTwbq5P8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_page_no = []"
      ],
      "metadata": {
        "id": "wfHeb9KB5iKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_image(array_np):\n",
        "  response_caption = model_gem_pro_vis.generate_content([\"Describe the photo/image in the file, also describe the text in the file\",Image.fromarray(array_np)])\n",
        "  try:\n",
        "    response_caption.resolve()\n",
        "    return response_caption.text\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "GCEwsmQIBlUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_image(page):\n",
        "\n",
        "    # Render the page as an image\n",
        "    mat = fitz.Matrix(2, 2)\n",
        "    page_image = page.get_pixmap(matrix=mat)\n",
        "    # page_image.save(f\"{name}_image.png\")\n",
        "\n",
        "    # Create a new image with the same dimensions\n",
        "    th = int(page_image.height * 0.05)\n",
        "    chth = page_image.height * 0.01\n",
        "    cwth = page_image.width * 0.01\n",
        "    cath = chth * cwth * 100\n",
        "    text_image = Image.new('RGB', (page_image.width, page_image.height), (0, 0, 0))\n",
        "    draw = ImageDraw.Draw(text_image)\n",
        "\n",
        "    # Extract the text blocks from the page\n",
        "    blocks = page.get_text(\"blocks\")\n",
        "\n",
        "    # Draw each block on the image\n",
        "    # print(len(blocks))\n",
        "    for block in blocks:\n",
        "        x, y, w, h, text, _, _ = block\n",
        "        draw.rectangle([2*x, 2*y, 2*w, 2*h], fill=\"white\")\n",
        "\n",
        "    # Save the text image\n",
        "    # original = Image.open(f\"{name}_image.png\")\n",
        "    maskArray = np.array(text_image)\n",
        "    # text_image.save(f\"{name}_text.png\")\n",
        "\n",
        "    # image = cv2.imread(f\"{name}_text.png\", 0)\n",
        "    image = cv2.cvtColor(maskArray, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresholded = cv2.threshold(image, 254, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresholded, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    rectangles = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        rectangles.append((x, y, w, h))\n",
        "\n",
        "    diagArray = np.maximum(maskArray, np.frombuffer(page_image.samples, dtype=np.uint8).reshape(page_image.h, page_image.w, page_image.n))\n",
        "\n",
        "    diagGray = cv2.cvtColor(diagArray, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh = cv2.threshold(diagGray, 250, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # print(contours)\n",
        "    selected = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        if w < cwth or h < chth or w*h < cath:\n",
        "            # maskArray[y: y + h, x: x + w] = 255\n",
        "            continue\n",
        "        # x -= th//2\n",
        "        y -= th//2\n",
        "        # w += th\n",
        "        h += th\n",
        "        temp = []\n",
        "        for block in rectangles:\n",
        "            x1 = int(block[0]) - th//2\n",
        "            y1 = int(block[1]) - th//2\n",
        "            w1 = int(block[2]) + th\n",
        "            h1 = int(block[3]) + th\n",
        "            # if y1 < y:\n",
        "            #     if y1 + h1 - y < th:\n",
        "            #         selected.append((x1, y1, w1, h1))\n",
        "            # else:\n",
        "            #     if y + h - y1 < th:\n",
        "            #         selected.append((x1, y1, w1, h1))\n",
        "            if ((y1 < y and y1+h1 > y) or (y < y1 and y+h > y1)) and (((x1 < x and x1+w1 > x) or (x < x1 and x+w > x1))):\n",
        "                temp.append((x1, y1, w1, h1))\n",
        "\n",
        "        # x *= 2\n",
        "        # y *= 2\n",
        "        # w *= 2\n",
        "        # h *= 2\n",
        "        # print(x, y, w, h)\n",
        "        temp.append((x, y, w, h))\n",
        "        # maskArray[y: y + h, x: x + w] = 0\n",
        "        # for i in range(x, x + w - 2):\n",
        "        #     for j in range(y, y + h - 2):\n",
        "        #         print(i, j)\n",
        "        #         # maskArray[i, j] = (0, 0, 0)\n",
        "        # print(maskArray.shape)\n",
        "        # print(sum(maskArray.flatten()))\n",
        "        selected.append(temp)\n",
        "\n",
        "    # cv2.drawContours(diagArray, contours, -1, (0,255,0), 3)\n",
        "\n",
        "    maskArray[0:page_image.height, 0:page_image.width] = 255\n",
        "    for sel in selected:\n",
        "        for i, temp in enumerate(sel):\n",
        "            x, y, w, h = temp\n",
        "        # x *= 2\n",
        "        # y *= 2\n",
        "        # w *= 2\n",
        "        # h *= 2\n",
        "            maskArray[y: y + h, x: x + w] = 0\n",
        "    diagArray = np.maximum(maskArray, np.frombuffer(page_image.samples, dtype=np.uint8).reshape(page_image.h, page_image.w, page_image.n))\n",
        "    diagram = Image.fromarray(diagArray)\n",
        "    ddraw = ImageDraw.Draw(diagram)\n",
        "    images = []\n",
        "    for i, sel in enumerate(selected):\n",
        "        if len(sel) == 0:\n",
        "            continue\n",
        "        imx, imy, imw, imh = sel[0]\n",
        "        for temp in sel:\n",
        "            x, y, w, h = temp\n",
        "            imx = min(imx, x)\n",
        "            imy = min(imy, y)\n",
        "        for i, temp in enumerate(sel):\n",
        "            x, y, w, h = temp\n",
        "            imw = max(imw + imx, w + x) - imx\n",
        "            imh = max(imh + imy, h + y) - imy\n",
        "            # ddraw.rectangle([(x, y), (x+w, y+h)], width=5, outline='green')\n",
        "        new = diagArray[imy:imy + imh, imx:imx + imw]\n",
        "        images.append(new)\n",
        "        # newImage = Image.fromarray(new)\n",
        "        # newImage.save(f\"{name}_{i}.png\")\n",
        "        # ddraw.rectangle([(imx, imy), (imx+imw, imy+imh)], width=5, outline='blue')\n",
        "    # diagram.save(f\"{name}_image.png\")\n",
        "    return images\n",
        "\n",
        "    # text_image = Image.fromarray(maskArray)\n",
        "    # text_image.save(f\"{name}_text.png\")\n",
        "\n",
        "\n",
        "def image_extractor(page):\n",
        "    images = create_text_image(page)\n",
        "    return images\n",
        "\n",
        "\n",
        "def relevant_images(relevant_page_no,query):\n",
        "  relevant_images = []\n",
        "  image_docs = []\n",
        "\n",
        "  for i in relevant_page_no:\n",
        "    page = fitz_doc[i]\n",
        "    image_r = image_extractor(page)\n",
        "    relevant_images.extend(image_r)\n",
        "\n",
        "\n",
        "  for i in range(len(relevant_images)):\n",
        "    try:\n",
        "      image_doc = Document(page_content = caption_image(relevant_images[i]),metadata={ 'index' : i})\n",
        "      image_docs.append(image_doc)\n",
        "      # print(\"image captioned\")\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "  faiss_index_images = FAISS.from_documents(image_docs,instructor_embeddings)\n",
        "\n",
        "  qa_chain_images = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    faiss_index_images.as_retriever(search_kwargs={'k': 5}),\n",
        "    return_source_documents=True,)\n",
        "  result_images = qa_chain_images({'question': query,'chat_history':\"\"})\n",
        "\n",
        "  to_return = []\n",
        "  # print(len(relevant_images))\n",
        "\n",
        "  for i in range(len(result_images['source_documents'])):\n",
        "    to_return.append(Image.fromarray(relevant_images[result_images['source_documents'][i].metadata['index']]))\n",
        "\n",
        "  return to_return\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FORR_5RP6TP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "relevant_page_no = []\n",
        "\n",
        "query = input('Prompt: ')\n",
        "if query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "\n",
        "result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "# print(result['source_documents'][0])\n",
        "\n",
        "\n",
        "for i in result['source_documents']:\n",
        "  relevant_page_no.append(i.metadata['index'])\n",
        "\n",
        "print('Most Relevant pages')\n",
        "count = 0\n",
        "for i in relevant_page_no:\n",
        "  block_texts = fitz_doc[i-1].get_text(\"blocks\")\n",
        "  for j in block_texts[::-1]:\n",
        "    j_4_replaced = j[4].replace('-','')\n",
        "    # print(j[4],j[4].count('-') == 1,j_4_replaced,len(j_4_replaced))\n",
        "    if j[4].count('-') == 1 and (j_4_replaced).strip().isnumeric():\n",
        "      count+=1\n",
        "      print(i,end = \" \")\n",
        "      print(j[4])\n",
        "      break\n",
        "  if count == 3:\n",
        "    break\n",
        "\n",
        "# print(result)\n",
        "# print(get_links(str(result['source_documents'][0])))\n",
        "# print(get_links(str(result['source_documents'])))\n",
        "# print(result['answer'])\n",
        "print(result['answer'][result['answer'].rfind('Helpful Answer:'):] + '\\n')\n",
        "chat_history.append((query, result['answer']))\n",
        "\n",
        "# print(list(set(relevant_page_no)))\n",
        "\n",
        "a = relevant_images(relevant_page_no[:2],query)\n",
        "\n",
        "for i in range(len(a)):\n",
        "  a[i].save('img'+str(i)+'.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UKoXyl3s5VYX",
        "outputId": "fc360b86-f3b2-415e-9473-f57c4bdbdbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: what is main switch/steering lock?\n",
            "Most Relevant pages\n",
            "28 4-2\n",
            "\n",
            "16 2-3\n",
            "\n",
            "29 4-3\n",
            "\n",
            "Helpful Answer: The main switch/steering lock is a control on your motorcycle that manages the ignition and lighting systems, and is used to lock the steering. It has different positions, such as \"ON\", \"OFF\", and \"LOCK\", each with its own function. It is important to use the standard key for regular use and keep the code-registering key in a safe place to minimize the risk of losing it. Additionally, it is crucial not to turn the key to \"OFF\n",
            "\n"
          ]
        }
      ]
    }
  ]
}